# OC
[![Build Status](https://travis-ci.org/abner-ma/OC.svg?branch=master)](https://travis-ci.org/abner-ma/OC)
---
Oh,the C program.Also mean Oh see!
test for myself.

URFS is a file system based on the NVMe protocol implemented in user-space. The main innovations include: user-space multi-process shared memory framework, which enables multiple APPs to access multiple NVMe SSDs. At the same time, the shared cache design improves the performance of APP accessing SSDs; minimalistic elastic layout, avoiding complex log mechanisms, taking into account both space utilization and read and write performance, and avoiding unnecessary data migration;elastic separation technology of multi granularity IO queue significantly reduces the average response time of IO requests.
After the user-space NVMe driver open the SSD device, it forms an exclusive ownership of SSD devices. As a result, multiple apps cannot share access to the SSD. When the No.2 APP needs to access the data of the No.1 SSD, a traditional method is to first unbind the No.1 APP from the No.1 SSD, then bind the No.2 APP to the No.1 SSD, and finally the No.2 APP accesses the No.1 SSD. Another traditional method is to send a message from the No.2 APP to the No.1 APP, and the No.1 APP sends the data of the No.1 SSD to the No.2 APP. Obviously, these two traditional methods are very inefficient.
Through the management of shared memory, this article realizes the shared access of multiple apps to multiple NVMe SSDs. At the same time, the shared cache design improves the performance of APP access to SSDs. As shown in Figure 1, the overall framework of URFS shared memory consists of APP, fslib, shared memory, fssvr, fs core, and SSD: 
?APP, it can be bound to one or more SSDs. Each APP can obtain a Handle of an NVMe SSD, and interact with fssvr through the Handle to actually complete the read and write operations.
?fslib,it is a client-side dynamic library that provides Posix file system APIs for client applications to call.
?fssvr, the server program fssvr, is responsible for access management of multiple NVMe devices. The fssvr main process is responsible for the management of NVMe SSD storage space, scheduling and sending read and write requests. Multiple APPs can share access to multiple NVMe SSDs by calling fslib. Multiple fslibs and fssvrs，through shared memory，exchange messages and transfer content.
?shared memory，it is the core of the framework, including five structures: Globalfd, Ctrl_head, CtrlBuf, Page_head, PageBuf.The messages between fslib and fssvr include control messages and data messages. The control messages are managed by the control header Ctrl_head and the control cache block CtrlBuf, and the data messages are managed by the page header Page_head and the page cache PageBuf. At the same time, Globalfd manages all file handles opened on the APP, and saves information about file opening, including file name, file id, storage location, storage block in operation, length notification and other information.
?fs core,it is the part that actually implements FS in user-space in URFS, and it contains user-space driver, implements the user-space NVMe protocol stack and is responsible for protocol interaction with the underlying SSD.
?SSD, each SSD is bound by a specific one or a group of APPs.
Among them, fslib will periodically,with fssvr, have heartbeat messages. For key resources such as Gloabalfd, control block header, and cache block, deadlock detection will be performed on the heartbeat situation and lock time. When it is detected that the APP has been abnormally closed or the resource is deadlocked, a forced release of the resource is performed. PageBuf can be shared between multiple application APPs. In this way, when one APP opens a file for reading and writing, other APPs can share access to PageBuf at the same time, reducing the amount of external storage access.
URFS copies the data in the SSDs accessed by each app for the first time into shared memory. For apps with similar access characteristics, most of the data is directly hit in memory, so the performance is very high. However, due to the limited space of shared memory, which is generally GB-level, and the capacity of a single SSD is TB-level, it is necessary to consider which data will be replaced from the memory after the shared memory reaches the capacity limit. A simple method is to use classic cache replacement algorithms such as LFU, LRU, ARC, FIFO, MRU, etc. However, these algorithms do not consider the problem of overlapping access hotspots in the scenario of multiple users. For example, a file has just been accessed by only one user,while another file is accessed by multiple users at the same time, even if the former is just accessed, the latter should have higher importance, because for shared memory proposed in this article, sharing is an important feature, and the file being accessed by sharing cannot simply be swapped out without visiting recently. Therefore, this paper proposes a multi-user multi-factor shared memory replacement algorithm. When the shared memory is full and some files need to be eliminated, the E value of the file is used as the basis, and the top N files are selected as the cache elimination object. The calculation is shown in formula (1):
Among them: Ej indicates the E value of the j-th file in shared memory. The larger the E vlaue, the more it should be swapped out; Sj indicates the size of the j-th file in shared memory; T indicates a period of time;tji indicates in shared memory, j-th files, in the T time period, in the i time, the most recently unused time; Rji represents the j-th file in the shared memory, in the T time period, the correlation coefficient at the i time, Rji = Mji/N, where N is the total number of APPs, and Mji is the number of APPs that have operations on the j-th file in time i.α indicates the LRU parameter, which is obtained based on historical experience or statistical results;β indicates the correlation parameter, which is obtained based on historical experience or statistical results; γ, which is a full correlation parameter, takes a small value to avoid 1-Rji being 0, and is obtained based on historical experience or statistical results.
In addition to using shared memory between apps, this article also proposes more fine-grained shared memory in NUMA to solve the high latency problem of accessing and accessing NVMe devices across NUMA memory. fssvr binds the working thread responsible for request processing to the CPU of the same NUMA node as the NVMe device. In addition, fssvr uses a globally pre-allocated memory pool of the same NUMA node as the data buffer. For a particular NVMe device, the working thread is unique. The API calls provided by fslib are eventually send request to a request queue, and the working thread is responsible for processing, and only read and write buffers are allocated from the same NUMA node memory pool. In this way, the Cache hit rate is improved, and operations on file metadata and data can also be completed without locks, reducing CPU overhead. The existence of the request queue also provides an opportunity for further IO optimization. In short, fssvr implements a simple and efficient request processing by allocating a working thread of the same NUMA to each NVMe SSD.
